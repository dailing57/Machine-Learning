# Logistic GD:
训练数据为前70个，测试数据为后10个

由于最后的测试数据中，没有y=0的情况，所以positive的精确率的分母为0

[ 0.98030633  0.32713126 -0.18527687]

loss: 0.7260251171240303

positive precision: 0.0

negative precision: 1.0

negative recall: 0.7

# Logistic SGD:
（多次随机中一次比较好的结果）

[ 0.51765598 0.01907184 -0.06438467]

loss: 0.0473754990284663

negative precision: 1.0

negative recall: 1.0


# Softmax GD:

[[1.00209912 0.98296408 1.0028643 ]

 [0.99784024 1.00636108 0.99637989]]

0.8328185076925413

positive precision: 0.0

negative precision: 1.0

negative recall: 0.8

# Softmax SGD:

[[1.00208178 0.98209633 1.00164888]

 [0.99783418 1.00595238 0.99684912]]

0.904410766916955

negative precision: 1.0

negative recall: 1.0

# 对比
两者都是处理分类问题，实际上softmax就是logistic的多维度版本，
logistic的区分两个种类只需要一个θ只有一个，
而softmax区分n个种类需要n个θ，哪个θ与当前这个x向量的h值（概率）最高
这个x就属于哪个种类
并且两种操作都可以使用梯度下降法进行优化。

对于GD与SGD，GD可以非常稳定的得到一个比较合理的答案，但是速度缓慢
而SGD可以快速得到结果，但是结果并不一定像想象种完美。不过SGD有可能
因为运气比较好而跃迁到一个GD难以收敛到的好结果。就实际应用而言，我们
是为了得到一个比较有效的θ，还是采用SGD多次运行取一个较好的结果为宜。
